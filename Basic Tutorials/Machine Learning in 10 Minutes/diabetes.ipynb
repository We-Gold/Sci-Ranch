{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Diabetes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESN4G72cllQc",
        "colab_type": "text"
      },
      "source": [
        "First, install OpenML, a library that provides access to the datasets on https://www.openml.org."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoigTRf5k3xN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install openml\n",
        "#!pip install tpot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA6nn2_nl_Nq",
        "colab_type": "text"
      },
      "source": [
        "Next, import `openml`. You also import `train_test_split` for splitting up the data into training and testing data. Also import `scale` to scale down all of the values so they have equal importance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trKORtQ3k-t7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import openml as oml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from tpot import TPOTClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPexJsoEmc64",
        "colab_type": "text"
      },
      "source": [
        "Next, load in the dataset from OpenML, scale it, and split it into training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VoJc-3TlAlD",
        "colab_type": "code",
        "outputId": "a3d895ca-7243-4d62-936a-6217c0d37d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "dataset = oml.datasets.get_dataset(37)\n",
        "\n",
        "x, y, attribute_names = dataset.get_data(\n",
        "    target=dataset.default_target_attribute,\n",
        "    return_attribute_names=True,\n",
        ")\n",
        "\n",
        "x = scale(x)\n",
        "\n",
        "xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2, random_state=42)\n",
        "\n",
        "print(x[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.6399472   0.8483238   0.14964074  0.90726995 -0.69289064  0.2040126\n",
            "  0.46849185  1.4259955 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
            "  warnings.warn(\"Numerical issues were encountered \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFyQIOf_mr9v",
        "colab_type": "text"
      },
      "source": [
        "Next, initialize the classifier, and fit it to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf4RFje7mWcL",
        "colab_type": "code",
        "outputId": "b5d78a85-756c-4ea6-ef12-0a2575ac0b91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(xtrain,ytrain)\n",
        "\n",
        "#tpot = TPOTClassifier(generations=5, verbosity=2)\n",
        "#tpot.fit(xtrain,ytrain)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
              "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
              "          verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHHN5gMZmzwN",
        "colab_type": "text"
      },
      "source": [
        "Finally, print out the accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyyZuqn_mcAv",
        "colab_type": "code",
        "outputId": "6f5dfe1a-93c9-4bfb-800a-4b8184e05de3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(model.score(xtest,ytest))\n",
        "\n",
        "#tpot.score(xtrain,ytrain)\n",
        "#tpot.export('pipeline.py')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7532467532467533\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}